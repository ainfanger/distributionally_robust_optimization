
\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,amsfonts,mathtools,cancel,enumerate,bm,float}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}


\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\ddt}{\frac{d}{dt}}
\newcommand\norm[1]{\left\lVert#1\right\rVert}

\renewcommand{\Pr}{\textbf{Pr}}
\newcommand{\E}{\textbf{E}}
\newcommand{\ind}{\textbf{I}}
\newcommand{\bin}{\textbf{binom}}
\newcommand{\Poisson}{\textbf{Poisson}}
\newcommand{\var}{\textbf{var}}
\newcommand{\cov}{\textbf{cov}}
\newcommand{\prightarrow}{\overset{p}{\to}}
\newcommand{\asrightarrow}{\overset{a.s.}{\to}}
\newcommand{\define}{\overset{\Delta}{=}}
\newcommand{\dist}{\overset{D}{=}}
\newcommand{\cond}{\textbf{cond}}



%Theorems, proofs and solutions.
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\theoremstyle{definition}
\newtheorem{lemma}{Lemma}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}
\newtheorem{proposition}{Proposition}
\newtheorem{scolium}{Scolium}   %% And a not so common one.
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{example}{Example}
\newenvironment{solution}
  {\begin{proof}[Solution]}
  {\end{proof}}





\setlength{\parindent}{0pt}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}

\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\makeatletter
\begin{document}


\title{Notes on Distributionally Robust Optimization for the calculation of Ruin Probabilities}
\author{Alex Infanger}
\maketitle
\emph{All of the following is based on or directly from \cite{JosePaper}.}\\

The following assumptions will be made throughout the work.
\begin{assumption} (A1) The cost function $c:S\times S\rightarrow\R^+$ is a non-negative lower semicontinuous function satisfying $c(x,y)=0$ if and only if $x=y$.
\end{assumption}
\begin{assumption} (A2) The function $f:S\rightarrow\R$ whose expectation we are interested in lives in $L^1(d\mu)$ and is upper semicontinuous.
\end{assumption}

We will take for granted the ``DRO duality'' proved in class for the compact $S$ case (with discussion over the extension). We restate it here.

\begin{theorem} \emph{(Strong Duality for DRO)} \label{strong_duality} Under the assumptions (A1) and (A2), we have that the primal problem
\begin{align*}
\begin{array}{ll@{}ll}
\displaystyle \sup   &\displaystyle \int_{}^{}f d\nu &\\
\text{s.t.}&  d_c(\mu,\nu)\leq \delta\\
                 & 
\end{array}
\end{align*}
is equal to the dual problem, 
\begin{align*}
\begin{array}{ll@{}ll}
\displaystyle \inf  &\displaystyle \lambda \delta + \int_{}^{}\phi d\mu &\\
\text{s.t.}& \lambda\geq 0 \\
          & \phi(x)+\lambda c(x,y) \geq f(y) \text{ for all $x,y$}
\end{array}
\end{align*}
(which upon noting the form of the optimizer can be written as a one dimensional problem)
\begin{align*}
\begin{array}{ll@{}ll}
\displaystyle \inf_{\lambda\geq 0}  &\displaystyle \lambda\delta +  \E_{\mu}\left[\sup_{y\in S} \left(f(y)-\lambda c(X,y)\right)\right]. 
\end{array}
\end{align*}
With this strong duality come the complementary slackness conditions,
\begin{align*}
&\lambda\left(d_c(\mu,\nu)-\delta\right)=0\\
&\phi(x)=\sup_y \left(f(y)-\lambda c(x,y)\right) \text{ $\pi^*$ a.s.}
\end{align*}
\end{theorem}

Our mission now is to simplify the above when we our function $f=\bm{1}_{A}$ is an indicator, so that we can calulate ruin probabilities. Let me extend our definition slightly from class,
\begin{align*}
c(x,A) = \begin{cases}
\inf\{c(x,y), y\in A\} & x\not\in A\\
0 & \text{otherwise}
\end{cases}.
\end{align*}


We begin by recognizing that
\begin{align*}
\sup_{y\in S}\left\{\bm{1}_A(y)-\lambda c(x,y)\right\}=\left(1-\lambda c(x,A)\right)^+,
\end{align*}
which is clear casewise. Notice the left hand side is bounded above by one because $\lambda,c\geq 0$. Then if $x\in A$ choose $y=x$ to achieve that bound. On the other hand if $x\not\in A$ the choice $y=x$ gets you zero, and the question is whether we can choose $y\in A$ that is $c-$close enough to $x$ to have $\bm{1}_A(y)-\lambda c(x,y)\geq 0$. hence we have,
\begin{align*}
\sup_{y\in S}\left\{\bm{1}_A(y)-\lambda c(x,y)\right\}&=\max\{1-\lambda c(x,A), 0\}=\left(1-\lambda c(x,A)\right)^+.
\end{align*}

Potentially, for the sake of time, skip the build up of theorem 3. 
\begin{theorem}\label{simplified_3} \emph{(Simplified Theorem 3)} Suppose that Assumption (A1) is in force. If $A$ is a nonempty closed subset of the Polish space $S$, and $\lambda\in[0,\infty)$ attains the infimum in Theorem \ref{strong_duality}, and the function,
\begin{align*}
h(u)\define\int_{\left\{x: c(x,A)\leq u\right\}}^{}c(x,A)d\mu(x)
\end{align*}
is continuous, we can guarantee
\begin{align*}
\sup\left\{P(A): d_c(\mu,P)\leq \delta\right\}=\mu\left\{ x: c(x,A)\leq \frac{1}{\lambda^*}\right\}.
\end{align*}
\end{theorem}

\clearpage


\section*{An Example}
We work here with the Cramer-Lundberg model for insurance claims. This is a continuous time stochastic process, where the amount of money in the bank $R(t)$ satisfies
\begin{align*}
R(t)=u+ct-\sum_{i=1}^{N_t}X_i.
\end{align*}
Here $u$ is the initial money in the bank, $c$ is the premium rate, and the $X_i$ are claim sizes. They come in at a rate that is a Poisson process, $N_t$. You have to assume the $X_i$ are distributed in some way. Let this distribution have first and second moments $m_1$ and $m_2$ respectively. We can assume $m_1<\infty$ else the insurance company would have no point insuring such a risk. Just for the insurance company to make even, the amount of money coming in has to be the amount of money being paid out on average, which corresponds to the choice
\begin{align*}
c=\nu m_1
\end{align*}
where $\nu$ is the rate of the Poisson process. To make money, the firm adds a safety loading -- the so-called risk premium -- so that,
\begin{align*}
c=(1+\eta)\nu m_1.
\end{align*}
Hence our final model is of the form,
\begin{align*}
R(t)=u+(1+\eta)\nu m_1t-\sum_{i=1}^{N_t}X_i.
\end{align*}
We are interested in the probability of ruin
\begin{align*}
\psi(u,T)\define\Pr\left[\inf_{t\in[0,T]} R(t)\leq 0\right].
\end{align*}
It turns out that it is computationally intractable to solve the above so we instead work with the following Brownian motion approximation,
\begin{align*}
R_B(t)&\define u+(1+\eta)\nu m_1t - (\nu m_1t + \sqrt{\nu m_2}B(t))\\
&=u+\eta \nu m_1t-\sqrt{\nu m_2}B(t).
\end{align*}
(There may be a typo here, double check if it's $\nu^2$). Our mission is to robustify this estimate by allowing for some non-trivial movement away from the Brownian motion. For this purpose, we identity the Polish space where the stochastic processes of interest live as the \emph{Skorkhod space},
\begin{align*}
S=D([0,T],\R)
\end{align*}
the space of real valued right-continuous functions with left limits equipped with the $J_1$ metric. 

\section*{The $J_1$ Metric}
The $J_1$ metric is supposed to be like a $\sup$ metric that is robust against time shifts that go to zero. Let $\Lambda$ be the set of strictly increasing and continuous functions $\lambda:[0,T]\rightarrow [0,T]$ such that $\lambda(0)=0$ and $\lambda(1) = 1$, and let $e$ be the identity map on $[0,T]$.  Then we define,
\begin{align}
d_{J_1}(x_1,x_2)&\define\inf_{\lambda\in \Lambda}\left\{\norm{x_1\circ \lambda -x_2}_{\infty}\lor \norm{\lambda-e}_{\infty}\right\}\label{J1Norm}
\end{align}
where $\lor$ is the max function.\\

Intuitively, this is suppose to capture the sup norm except you're allowed to perturb the function input a litte bit. So it's a bit weaker than the sup norm. In fact, plugging in $\lambda=e$ in Equation \ref{J1Norm} tells us that everything is closer together with the $J_1$ metric compared to the $\sup$ metric. Take for example when $T=1$,
\begin{align*}
x_n=(1+n^{-1})\bm{1}_{\left\{[0,2^{-1}+n^{-1})\right\}}, \ \ x = \bm{1}_{\left\{[0,2^{-1})\right\}}.
\end{align*}
Then $\norm{x_n-x}_{\infty}\geq 1$ for all $n$ but $d_{J_1}(x_n,x)\rightarrow 0$ as $n\rightarrow\infty$. To show this notice that,
\begin{align*}
d_{J_1}(x_n,x)&=\inf_{\lambda\in \Lambda}\left\{\norm{x_n\circ \lambda - x}_\infty \lor \norm{\lambda-e}_{\infty}\right\}\\
&\leq \max\left\{\norm{x_n\circ \lambda_n-x}_\infty, \norm{\lambda_n-e}_{\infty}\right\},
\end{align*}
for some feasible choices $\lambda_n$. Let us choose 
\begin{align*}
\lambda_n&=\begin{cases} (1+2n^{-1})t & t\in [0,2^{-1}]\\
(1-2n^{-1})t + 2n^{-1}& t\in [2^{-1},1].
\end{cases}
\end{align*}
Notice that the difference between $x_n\circ \lambda_n$ and $x$ sees contribution from the part of the domain $t\in[0,\frac{1}{2})$, so that,
\begin{align*}
\norm{x_n\circ \lambda_n-x}_{\infty}= \frac{1}{n}.
\end{align*}
On the other hand we have that the identity map and $\lambda$ differ the most at the point $x=\frac{1}{2}$ where they differ by $\frac{1}{n}$, so that we conclude,
\begin{align*}
d_{J_1}(x_1,x_2)&=\inf_{\lambda\in \Lambda}\left\{\norm{x_n\circ \lambda}_\infty \lor \norm{\lambda-e}_{\infty}\right\}\\
&\leq \max\left\{\norm{x_n\circ \lambda_n}_\infty, \norm{\lambda_n-e}_{\infty}\right\}\\
&\leq \frac{1}{n}\rightarrow0.
\end{align*}
For readability we follow the convention of Billingsley $\lambda t\define \lambda(t)$. 
\begin{theorem} Noting the above, a good characterization of convergence in $J_1$ is that,
$d_{J_1}(x_n,x)\rightarrow 0 $ if and only if you can find some sequence $\lambda_n$ such that $\lim_n (x_n(\lambda_n(t))\rightarrow x(t)$  uniformly in $t$ and $\lambda_nt$ converges uniformly to $t$.
\end{theorem}

On uniformly continuous functions, Skorokhod convergence is equivalent to uniform convergence. From the triangle inequality, 
\begin{align*}
|x_n(t)-x(t)|\leq |x_n(t)-x(\lambda_n t)|+ |x(\lambda_n t)-x(t)|,
\end{align*}
which means if $d_{J_1}(x_n,x)\rightarrow 0$ then the left hand term goes to zero uniformly in $t$. The right hand term goes to zero uniformly in $t$ if we assume $x$ is uniformly continuous.\\

[If there is time put in the $j(x)$ example].



\clearpage
\section*{Back to the Process of Robustification}
We intend to invoke Theorem \ref{simplified_3} so that we can show that our robust ruin calculation simplifies to a standard Brownian motion ruin calculation with a different initial money-in-the-bank $u$. The set of interest is the ruin set,
\begin{align*}
A_u\define \left\{x\in S: \sup_{t\in[0,T]} \left(\sqrt{\nu m_2}x(t)-\nu m_1t\right)\geq u\right\}.
\end{align*}
In order to invoke Theorem \ref{simplified_3}, we must show that the above is closed. Before that we will want the following simple proposition,
\begin{proposition} \label{prop_inequality} The following inequality holds
\begin{align*}
\sup_{t\in[0,T]}y(t)\leq \sup_{t\in[0,T]} x(t)+d_{J_1}(x,y).
\end{align*}
\end{proposition}
\begin{proof} The point is that we want to switch the well known inequality,
\begin{align*}
|\sup_t y(t)-\sup_t x(t)|\leq \norm{x-y}_{\infty}
\end{align*}
with the slightly stronger statement,
\begin{align*}
|\sup_t y(t)-\sup_t x(t)|\leq \norm{x-\lambda y}_{\infty} \text{ for all $\lambda\in \Lambda$}.
\end{align*}
The desired conclusion will follow from taking an infimum over $\lambda$. Hence we follow a similar path to that of the former inequality. Notice that,
\begin{align*}
f(t)&\leq \sup_t f(t) \text{ for all $t$}\\
g\lambda(t)&\leq \sup_t g(t) \text{ for all $t,\lambda$},
\end{align*}
from which we conclude,
\begin{align*}
\sup_t\left(f(t)+g\lambda(t)\right)\leq \sup_t f(t)+\sup_t g(t).
\end{align*}
For the choice $f=f-g\lambda$ we achieve,
\begin{align*}
\sup_t f(t)-\sup_t g(t)\leq \sup_t (f-g\lambda), \text{ for all $\lambda$}.
\end{align*}
Finally we have that,
\begin{align*}
\sup_t f(t)-\sup_t g(t)\leq \inf_{\lambda}\norm{f-g\lambda}_{\infty}\leq d_{J_1}(f,g).
\end{align*}
\end{proof}
In the Appendix, we consider the more general set 
\begin{align*}
A_u = \left\{x\in D\left([0,T],\R\right): \sup_{t\in[0,1]} x(t)\geq u\right\},
\end{align*}
and prove $A_u$ is closed.

\begin{lemma}
The set $A_u\subseteq S$ is closed in the $J_1$ metric. 
\end{lemma}
\begin{proof}
We will show the compliment is open. Pick any $x$ in the compliment, and let 
\begin{align*}
\epsilon \define \frac{\left(u-\sup_{t\in[0,T]} x(t)\right)}{2}>0.
\end{align*}
Now consider some $y\in S$ which is within $\epsilon$ of $x$. We have using proposition \ref{prop_inequality}
\begin{align*}
\sup_{t\in [0,T]}y(t)\leq \sup_{t\in[0,T]}x(t) +d_{J_1}(y,x) < \sup_{t\in[0,T]}x(t)+ \epsilon<u.
\end{align*}
Hence we have found an open ball around $x$ that is contained in $A_u$. 
\end{proof}

\begin{lemma} The minimum cost to the set $A_u$ satisfies,
\begin{align*}
c(x,A_u) \define\inf\left\{c(x,y),y\in A_u\right\} = \left(u-\sup_{t\in[0,T]}\left(\sqrt{\nu m_2}x(t)-\eta \nu m_1t\right)\right)^p
\end{align*}
\begin{proof}
In Appendix of paper. To be added.
\end{proof}
\end{lemma}

\begin{lemma} The function,
\begin{align*}
h(s)=\E_{\mu}\left[c(X,A); c(x,A)\leq s\right]
\end{align*}
is continuous. 
\end{lemma}
\begin{proof} Supposedly it is clear by inspection.
\end{proof}

With the above statements you can identify $\lambda^*=h^{-1}(\delta)=\inf\left\{s\geq 0 : h(s)\geq \delta\right\}$. We can use Theorem \ref{simplified_3} to show,
\begin{align}
\psi_{\text{rob}}(u,T)= \Pr\left[\sup_{t\in[0,T]} \left(\sqrt{\nu m_2}x(t)-\eta \nu m_1t\right)>u -\left(\frac{1}{\lambda^*}\right)^{1/p}\right] = \psi_B(\tilde u, T),\label{Brownian_robust}
\end{align}
and so finding the robust ruin probability just settles down to finding a different Brownian motion probability. 

\section*{Numerical Example}
In the numerical example from Jose Blanchet and Karthyek Murthy, they use the Paretto distribution for the claim sizes, with $\alpha=2.2$. One thing we need to do is to pick $\delta$. Ideally, we pick $\delta$ just big enough to capture the distance from the Brownian motion to our process. We approximate the optimal coupling by finding a good coupling between the Brownian motion and the Poisson process. The coupling offered by Khoshnevisan simulates a coupled Brownian motion and Poisson process so that they are very correlated. Hence we seem to assume the cost function is supermodular. With this embedding we can estimate the cost $\delta$ with Monte Carlo, and then use this to find $\lambda^*$. At this point we generate a new Brownian motion according to Equation \ref{Brownian_robust}. 


\section*{Questions and things still to solve}
\begin{enumerate}
\item I am worried that I might have implemented that Khosnevisan embedding incorrectly for the following reason: I have to simulate more Brownian motion time steps than Poisson timesteps. This stems from the fact that 
\begin{align*}
\sigma(t)\define\inf\{s: A(s)=m_1t\}.
\end{align*}
But $A(t)=N(t)+S(t)$ is effectively the $\sup$ of a Brownian motion. Because of this it goes like $\sqrt{t}=o(t)$ such that the above equality (or in the computer it is a $\geq$) exists only for $t$ less than that simulated.

\item How would I implement the large deviations simulation?

\item The uncountable inf of lower semi-continuous is not necessarily lower semicontinuous. How do I show that for our case,
\begin{align*}
h(u)=\int_{\left\{x:c(x,A)\leq u\right\}}^{}c(x,A)d\mu(x)
\end{align*}
is continuous? This is a stochastic integral in our case.
\item  I seemed to have disproven the fact that $A_u$ is closed; where is my mistake or is this a typo?
\item It seems to me that the Khoshnevisan embedding is trying hard to correlate the two processes: does this mean we are assuming the cost function is supermodular?

\end{enumerate}

\begin{thebibliography}{9}
\bibitem{JosePaper} Jose Blanchet, Karthyek R.A. Murthy. ``Quantifying Distributional Model Risk via Optimal Transport.'' https://arxiv.org/abs/1604.01446.
\end{thebibliography}



\end{document}

